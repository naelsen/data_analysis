---
title: "Projet Analyse de données"
author: "RAHBI Aissam & SENNOUN Naël"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 4
---

```{r, echo=FALSE}
# Reset de l'environnement
rm(list=ls())
# Import des librairies nécessaires pour les analyses.
library(FactoMineR)
library(MASS)
library(klaR)
library(rmarkdown)
library(factoextra)
library(cluster)
library(rpart)
library(rpart.plot)
library(randomForest)
library(pROC)
```


# Introduction des données

Le jeu de données qui sera étudié dans ce projet décrit le relevé de satisfaction de passagers en avion. Il contient les notes attribuées (entre 0 et 5) pour différents services ainsi que les catégories et profil des voyageur.
Tout ceci dans le but de prédir leur satisfaction.

On voit que les deux premières colonnes (X & id) sont inutiles pour nos analyses, on va donc les supprimer.

```{r}
DF.train <- read.csv("data/train.csv")
DF.test <- read.csv("data/test.csv")
# On supprime les lignes ou il y a des valeurs manquantes.
DF.train <- DF.train[!rowMeans(is.na(DF.train)*1) > 0,]
DF.test <- DF.test[!rowMeans(is.na(DF.test)*1) > 0,]
str(DF.train)
```

```{r}
# On retire les colonnes inutiles.
DF.train <- DF.train[,c(-1,-2)]
DF.test <- DF.test[,c(-1,-2)]
# On modifie le nom des lignes pour le style ;-)
row.names(DF.train) <- paste("n°", sep="", 1:dim(DF.train)[1]) 
row.names(DF.test) <- paste("n°", sep="", 1:dim(DF.test)[1]) 
# On renomme les colonnes pour avoir des noms moins longs (Utile pour l'affichage).
colnames(DF.train) <- c("Genre", "Fidélité", "Age", "Type.du.vol", "Classe", "Distance", "Wifi", "Horaire.pratique", "Facilité.resevation", "Emplacement.porte", "Nourriture", "Enregistrement.en.ligne", "Siege.confort", "Loisir", "On.board.service", "Espace.jambe", "Gestion.bagage", "Checkin.service", "Inflight.service", "Propreté", "Retard.depart", "Retard.arrivé", "Satisfaction")
colnames(DF.test) <- c("Genre", "Fidélité", "Age", "Type.du.vol", "Classe", "Distance", "Wifi", "Horaire.pratique", "Facilité.resevation", "Emplacement.porte", "Nourriture", "Enregistrement.en.ligne", "Siege.confort", "Loisir", "On.board.service", "Espace.jambe", "Gestion.bagage", "Checkin.service", "Inflight.service", "Propreté", "Retard.depart", "Retard.arrivé", "Satisfaction")
```

Petit avant gout des données :

```{r, echo=FALSE}
paged_table(DF.train)
```

* Il y a autant d'hommes que de femmes dans notre jeu de données.

* Il y a une majorité de personnes qui voyagent pour une raison professionnelle que personnelle.

* Il y a beaucoup plus de clients loyaux que non loyaux.

* Les voyageurs en classe Éco Plus représentent une petite partie des voyageurs, quant aux classes business et éco ils en représentent un peu moins de la moitié.

* Pour finir, la proportion de voyageurs neutres ou insatisfaits et légèrement supérieure à celle des voyageurs satisfaits.

```{r, echo=FALSE}
# On affiche les proportions des modalités sous forme de camembert.
par(fig=c(0,1/2,1/2,1))
pie(table(DF.train$Genre), col = c("pink","blue"),cex=0.8)
par(fig=c(0,1/2,0,1/2),new=TRUE)
pie(table(DF.train$Fidélité), col = c("red","green"),cex=0.8)
par(fig=c(1/2,1,1/2,1),new=TRUE)
pie(table(DF.train$Type.du.vol), col = rainbow(2), cex=0.8)
par(fig=c(1/2,1,0,1/2),new=TRUE)
pie(table(DF.train$Classe), col = rainbow(3),cex=0.8)
par(fig=c(1/3-1/5,2/3+1/5,1/3-1/5,2/3+1/5),new=TRUE)
pie(table(DF.train$Satisfaction), col=c("red","green"), cex=0.8)
```

ON s'intéresse aux notes attribuées par les passagers. Ci-dessous on voit que les notes sont assez homogènes, elles ont toutes :

* Un 1er quartile égal à 2 ou 3.

* Une médiane égal à 3 ou 4.

* Un 3-ème quartile égale à 4 ou 5.

* Les moyennes pour les notes vont de 2.73 à 3.64.

```{r, echo=FALSE}
summary(DF.train[,7:20])
```

On peut aussi observer les autres variables quantitatives :

* À vu d'oeil, l'âge suit une loi Gaussienne centrée autour de 40  (bien qu'elle ne prenne que des valeurs entières dans le jeu de données, la variable est Gaussienne dans la vraie vie).

* La distance est une variable qui possède quelques valeurs extrêmes au dessus de 4000km.

* Pour les retards il y a énormement de données abérrantes.

```{r,echo=FALSE}
par(mfrow=c(2,2))
boxplot(DF.train$Age, main="Âge")
points(mean(DF.train$Age), col='red')
boxplot(DF.train$Distance, main="Distance")
points(mean(DF.train$Distance), col='red')
boxplot(DF.train$Retard.depart, main="Retard depart")
points(mean(DF.train$Retard.depart), col='red')
boxplot(DF.train$Retard.arrivé, main="Retard arrivée")
points(mean(DF.train$Retard.arrivé), col='red')
```


# Analyse des données

On commence notre analyse par une ACP sur les variables quantitatives.

## Méthodes fatorielles

### ACP

On récupère toutes les données quantitatives et la satisfaction.

```{r}
# On récupère Âge/Distance du vol/Retard Depart/Retard Arrivé (colonne 3/6/21/22), les notes (colonnes 7 à 20) et la satisfaction (colonne 23)
DF.tmp <- DF.train[,c(3,6,21,22,7:20,23)]
res.pca <- PCA(DF.tmp, quanti.sup=1:4, quali.sup=19, graph=FALSE)
```

Il est évident de penser que plus les notes attribuées par les voyageurs sont hautes et plus l'individu a de chance d'être satisfait, on peut le voir avec le graphe ci-dessous. Pour ne pas attribuer trop d'importance aux moyennes des notes des individus qui constituent un petit groupe, on associe une largeur par bande proportionnelle à la taille du groupe pour chaque moyenne attribuée.

```{r, echo=FALSE}
# On recupère les notes dans le dataframe, et on en fait la moyenne.
mean_notes <-rowMeans(DF.train[,7:20])
# Table de contingence pour les notes moyennes et la satisfaction.
tmp <- table(as.factor(mean_notes),DF.train$Satisfaction)
barplot((tmp/rowSums(tmp))[,2], # Fréquence de satisfaction.
         width=rowSums(tmp),    # Taille des groupes.
         col=rainbow(n = dim(tmp)[1],alpha=0.8,start=0,end=0.4),
         main = "Fréquence de satisfaction en fonction de la moyenne des notes",
         xlab = "Note moyenne",
         ylab = "Pourcentage d'individus satisfaits")
```

Ainsi, on va faire notre ACP sur les notes.

***

**Les individus ou variables peuvent être proches dans le plan mais eloignés dans l'espace s'ils sont mal représentés dans le plan.**

Ainsi, il est important d'expliquer le modèle avec des variables bien representées dans le plan.
Pour cela on veillera à ne pas prendre les variables et individus ayant un cos2 trop bas. Ici on choisit un cos2 égale à 0.68 afin qu'il ne soit pas trop bas et qu'on ait au moins 3 variables à utiliser afin d'expliquer nos individus sur les axes 1 et 2.

Ici l'axe 1 va opposer le confort à bord (droite) et l'inconfort (gauche), tandis que l'axe 2 fera le contraste sur les aspects techniques pour ce qui concerne le vol.

On peut voir que les variables quantitatives supplémentaires ne sont pas du tout interprétables (flèches en bleu) .

```{r}
plot(res.pca,select="cos2 0.68", choix="varcor")
```

**Lorsque l'angle entre deux axes est proche de 0°, les variables sont fortement corrélés positivement.**

**Lorsque l'angle est proche de 90°, les variables ne sont pas corrélés, ou très peu.**

**Lorsque l'angle est proche de 180°, les variables sont fortement corrélés négativement.**

* Ainsi, ci-dessus on voit que la facilité de reservation et le wifi sont fortement correlés.

* Loisir et Facilité.reservation ne le sont pas, ou très peu.

* Loisir et Wifi sont un peu corrélés positivement.

Verifions le :

Graphiquement (Facilité.resevation/Wifi) :

```{r,echo=FALSE}
boxplot(DF.tmp$Facilité.resevation ~ DF.tmp$Wifi,
        main = "Corrélation Facilité de reservation / Wifi",
        xlab = "Notes Wifi",
        ylab = "Facilité.reservation")
points(tapply(DF.tmp$Facilité.resevation, DF.tmp$Wifi, mean), col='red')
```

On remarque sur le boxplot qu'il semble y avoir une corrélation entre les notes attribuées à Wifi et à la Faclité de réservation, nous allons maintenant le vérifier.

**Test du chi-deux (Facilité.resevation/Wifi)** :

On rejette $H_0$ car p-value<0.5 , ainsi on peut affirmer avec un risque de se tromper de 5% que ces deux variables sont corrélés.

```{r}
chisq.test(DF.tmp$Facilité.resevation, DF.tmp$Wifi, simulate.p.value=TRUE)
```
Graphiquement on a pu voir qu'un passager donne, en moyenne, des notes égales à Facilité de reservation et Wifi.

***

Graphiquement (Loisir/Wifi) :

```{r,echo=FALSE}
boxplot(DF.tmp$Loisir ~ DF.tmp$Wifi,
        main = "Boxplot des notes Loisir / Wifi",
        xlab = "Notes Wifi",
        ylab = "Notes Loisir")
points(tapply(DF.tmp$Loisir, DF.tmp$Wifi, mean), col='red')
```

La corrélation entre Wifi et Loisir semble bien moins flagrante que celle vu précédemment, nous allons le  verifier.

**Test du Chi-deux (Loisir/Wifi) :**

On rejette $H_0$ car p-value<0.5 , ainsi on peut affirmer avec un risque de se tromper égal à 5% que ces deux variables sont corrélés.

```{r}
chisq.test(DF.tmp$Loisir, DF.tmp$Wifi, simulate.p.value=TRUE)
```

Graphiquement on a pu voir qu'un passager donne, en moyenne, des notes plus élevées au loisir quand il note bien le Wifi.

***

Graphiquement (Loisir/Facilité.reservation) :

```{r,echo=FALSE}
boxplot(DF.tmp$Loisir ~ DF.tmp$Facilité.resevation,
        main = "Boxplot des notes Loisir / Facilité reservation",
        xlab = "Notes Facilité de reservation",
        ylab = "Notes Loisir")
points(tapply(DF.tmp$Loisir, DF.tmp$Facilité.resevation, mean), col='red')
```

On vérifie une nouvelle fois la corrélation entre les deux variables.

**Test du Chi-deux (Loisir/Facilité.resevartion) :**

On rejette $H_0$ car p-value<0.5 , ainsi on peut affirmer avec un risque de se tromper égal à 5% que ces deux variables sont corrélés.

```{r}
chisq.test(DF.tmp$Loisir, DF.tmp$Facilité.resevation , simulate.p.value=TRUE)
```

Le test du chi-deux montre que Loisir et Facilité.reservation sont corrélés, mais on peut voir graphiquement qu'elles ne le sont pas vraiment, on peut faire l'hypothèse que c'est parce qu'un passager qui met de bonnes notes en moyenne aura tendance à mettre de meilleures notes aux autres catégories, on peut faire le même raisonnement pour les mauvaises notes.

***

Ci-dessous on peut voir une nette séparation entre les individus satisfaits et non satisfaits. Les individus satisfaits sont ceux s'étant amusé. À l'inverse les individus non satisfaits ont octroyé des notes plus basses concernant les loisirs. On ne peut pas vraiment dire pour l'instant si la facilité de réservation et le wifi à bord influent sur la satisfaction du passager.

Ici par exemple l'individu n°70657 est satisfait, or il a mal noté les loisirs, le wifi et la facilité de réservation. Sa satisfaction est probablement influencée par d'autres variables, qui sont qualitatives. On se penchera sur ce sujet lors de l'ACM. 

```{r}
plot(res.pca,habillage=19, select="cos2 0.93", choix="ind",cex = 0.8)
```

L'individu qui contribue le plus à l'axe 1 y contribue à environ 0.01%.

```{r}
max(res.pca$ind$contrib[,1])
```

Il n'y a pas d'individus atypiques, ce qui est normal car nos données sont des notes comprises entre 0 et 5 et on a vu qu'elles étaient homogènes.

***

Observons maintenant quelle part des données expliquent les composantes.

Premièrement, on peut voir  que les 3 premiers axes expliquent bien l'inertie sur les données : Les axes étant orthogonaux, ils expliquent 27.14% + 16.87% + 15.47% = 59.48% de l'inertie.

Quant aux axes 1 et 2, ils prennent en compte 27.14% + 16.87% = 44.01% du jeu de données.

On peut aussi voir que l'axe 2 et 3 expliquent à peu près autant l'un que l'autre l'inertie, ainsi on pourra aussi visualiser les données projetées sur le plan formé par l'axe 1 et 3 (voir 2 et 3).

```{r, echo=FALSE}
barplot(res.pca$eig[,2], col=rainbow(n=14,alpha=0.6,start=0,end=0.33),main="Pourcentage d'inertie expliquée par chaque axe", ylab="Contribution en %")
lines(seq(0.75,16.3,(16.3-0.75)/13),res.pca$eig[,2],type="b", xlim=c(0,max(res.pca$eig[,2])+10))
text(seq(0.75,16.3,(16.3-0.75)/13),res.pca$eig[,2]-1, paste(round(res.pca$eig[,2],2),"%"), cex=0.7)
```

On a un nouveau cercle de corrélation, ici on voit que :

* Le confort des sièges, la nourriture et la propreté sont fortement corrélés.

* La gestion des bagages et les services proposés sont fortement corrélés.

* Dans ce plan les loisirs sont presque totalement expliqués par l'axe 1.

* L'axe 3 oppose le "confort" aux services qui sont faiblement corrélés dû à l'angle proche de 90 degrés

```{r}
plot(res.pca, select="cos2 0.60", choix="varcor", axes = c(1,3))
```

Ci-dessous on voit donc que les passagers sont en général plus satisfait quand ils se sont amusé (loisirs), que le service était agréable (Gestion bagage, Inflight.service, On.board.service) et qu'ils ont passé un vol confortable (Siege.confort, nourriture, propreté), et inversement pour les passagers non satisfaits.

```{r}
plot(res.pca, habillage=19, select="cos2 0.9", cex=0.8, choix="ind", axes = c(1,3))
```

Ici on a le cercle de corrélation avec l'axe 2 et 3:

```{r}
plot(res.pca, select="cos2 0.6", choix="varcor", axes = c(2,3))
```

Il n'y a pas grand-chose à interpréter, car le barycentre des voyageurs satisfaits et non satisfaits sont tous les 2 proches du centre de gravité de ce plan.

```{r}
plot(res.pca, invisible="ind", choix="ind", axes=c(2,3))
```


#### Transformations & double centrage


**Certains passagers ont attribué comme notes 0 pour certaines catégories du vol, on décide de ne pas faire de transformation par l'inverse, car $\frac{1}{0}$ est un quotient indéterminé, ni de log-transformation car $\log(0)$ est indéterminé.**


***

On effectue la transformation ”double centrage” sur les données transformées par racine carrée afin de voir si on peut faire gagner en contribution les premières composantes afin d'être plus précis lors de nos analyses.

```{r}
# On récupère le notes dans le dataframe.
DF.tmp <- sqrt(DF.train[,7:20])
# On centre et on réduit par rapport aux colones.
DF.tmp <- scale(DF.tmp)
# ACP sur les données transformées.
res.pca <- PCA(DF.tmp, graph=FALSE)
barplot(res.pca$eig[,2], col=rainbow(n=14,alpha=0.6,start=0.66,end=1),
        main="Pourcentage d'intertie expliquée par chaque axe\n(Données transformées par racine carrée)",
        ylab="Contribution en %")
lines(seq(0.75,16.3,(16.3-0.75)/13), res.pca$eig[,2], type="b")
text(seq(0.75,16.3,(16.3-0.75)/13), res.pca$eig[,2]-1, paste(round(res.pca$eig[,2],2),"%"), cex=0.7)
```

Les transformations ne nous ont pas fait gagner plus d'informations au niveau des 3 premiers axes, on s'arrête là pour l'ACP.


#### Conclusion ACP


Ici, on a pu étudier l'impact des variables quantitatives sur la satisfaction des gens pour un trajet en avion.
Il en est ressorti que les services proposés dans l'avion et le confort à bord sont des aspects primordiaux pour la satisfaction des voyageurs, mais que cela n'expliquait pas totalement les données.


### ACM

Intéressons-nous maintenant aux données qualitatives.

```{r, echo=FALSE}
DF.tmp<- DF.train[,c(1,2,4,5,23)]
paged_table(DF.tmp)
```

On effectue une ACM :

```{r}
res.mca <- MCA(DF.tmp, quali.sup = 5, graph=FALSE)
```

On peut observer que la première dimension influe à un peu plus de 30%, quant aux dimensions 2, 3, 4, elles influent toutes autour de 20%.
Il sera donc sûrement nécessaire de s'intéresser à ces 4 dimensions.

```{r, echo=FALSE}
barplot(res.mca$eig[,2], col=rainbow(5))
```

```{r}
par(mfrow = c(2,2))
for(i in 1:4){
  barplot(res.mca$var$cos2[,i], las = 2, cex.names = 0.64, 
          col=rainbow(n=9,alpha=0.6,start=(i-1)/4,end=i/4),
          main=paste("Cos2 des modalités pour l'axe",i))
}
```

En se référant aux cos2 (cos2 > 0.7) des barplots précédents :

L'axe 1 oppose les passagers qui voyagent pour le business et les passagers qui voyagent pour un motif personnel. Ceux qui voyagent pour le travail sont en général plus satisfaits et ceux qui voyagent pour un motif personnel sont en général sans avis ou insatisfaits.

L'axe 2 oppose les passagers loyaux aux non-loyaux, il en résulte que les passagers non-loyaux sont plus souvent insatisfaits et que les passagers loyaux.

Les 2 axes prennent aussi en compte à eux deux les voyageurs qui voyagent en business et en éco.

On peut voir que les voyageurs en éco sont moins souvent satisfaits de leur voyage que ceux qui sont en business classe.

```{r}
plot(res.mca, invisible="ind", axes=c(1,2))
```

En se référant aux cos2 (cos2 > 0.7) pour l'axe 3 des barplots précédents :

On peut voir que l'axe 3 oppose les personnes de sexe diffèrent, on ne peut pas dire grand-chose quant à l'influence sur la satisfaction du client, on pourrait peut-être dire que les femmes sont un peu moins satisfaites, mais c'est à vérifier.

Vérifions le graphiquement:

Sur le barplot on voit donc que, pour notre jeu de données, les hommes sont un peu plus satisfait en général que les femmes. Mais rien de significativement très différent.

```{r}
tab = table(DF.train$Genre, DF.train$Satisfaction)
barplot(tab/rowSums(tab), beside=TRUE, col=c("pink","blue"), legend.text=c("Female","Male"))
```

```{r}
plot(res.mca, invisible="ind", axes=c(1,3))
```

Pour prendre en compte les voyageurs en classe éco plus on visualise sur le plan formé par l'axe 3 et 4.

On peut voir ici que les voyageurs en classe éco plus sont moins satisfaits de leur voyages, ils sont très excentrés car ils représentent des données inhabituelles.


```{r}
plot(res.mca, invisible="ind", axes=c(3,4))
```

Rappelez-vous de l'individu n°70657, il était satisfait de son vol malgré les mauvaises notes attribuées aux différentes catégories. On voit qu'il possède toutes les modalités influant positivement sur la satisfaction. Ceci montre bien que l'analyse des données qualitatives est importante, car on a eu des informations qu'on ne pouvait pas avoir avec l'ACP.

```{r, echo=FALSE}
paged_table(DF.tmp[70657,])
```


#### Conclusion ACM


D'après l'ACM :

* Si un passager voyage pour des raisons personnelles, il a moins de chance d'être satisfait du vol qu'un passager qui voyage pour le business.

* Si le passager est un client fidèle (Loyal custommer) il a plus de chance d'être satisfait du vol qu'un passager qui est déloyal (Disloyal custommer).

* Si un voyageur est en classe business, cela influera positivement sur sa satisfaction, alors que s'il est en classe Eco cela influera négativement. Les passagers Éco plus sont particuliers, mais le fait d'être en classe Éco plus influe négativement sur leur satisfaction.

* Le sexe de l'individu n'a pas l'air d'influer significativement sur la satisfaction, mais on peut faire l'hypothèse que les femmes sont moins satisfaites que les hommes.



## Classification non-superivée

Nous allons maintenant appliquer les méthodes de clustering vues en cours. Le regroupement "d'individus" ne sera pas très intéressant à réaliser, nous allons plutôt définir des profils avec l'ensembles des modalités. Ces profils pourront être regroupés en clusters. 

Dans cette partie, on considère l'âge comme une variable qualitative à quatres modalités :

* 7-19 ans
* 20-38 ans
* 39-60 ans
* 61+ ans

Avant la transformation, on vérifie si l'âge a un impact sur la satisfaction des passagers.

```{r, echo=FALSE}
# On recupère l'âge et la satisfaction des individus.
tab = table(data.frame(DF.train$Age,DF.train$Satisfaction))
# Table de contingence de l'âge et la satisfaction.
barplot(tab[,2]/(tab[,1]+tab[,2]), # Fréquence de satisfaction
        width=tab[,1]+tab[,2], # Taille des groupes
        col=rainbow(80),
        xlab="Age",
        ylab="Fréquence de Satisfaction",
        main="Fréquence de satisfaction selon l'age")
```

```{r, echo=FALSE}
# On définit la fonction de transformation
change_age <- function(x){
  if(7 < x && x <= 19)
    return("7-19")
  else if(19 < x && x <= 38)
    return("20-38")
  else if(38 < x && x <= 60)
    return("39-60")
  else
    return("61+")
}
DF.tmp = DF.train[,c(2:5)]
# On applique la fonction change_age sur chaque ligne de la colonne Age.
DF.tmp$Age = paste(lapply(DF.tmp$Age,change_age))
```

On a un nouveau dataframe pour les catégories d'âge comme variable qualitative, il nous permettra de créer les différents profils :

```{r, echo=FALSE}
paged_table(DF.tmp)
```

Nous allons maintenant regrouper les individus par groupes en utilisant les différentes méthodes de clustering. On pourra de cette manière déterminer quels sont les profils les plus proches en terme de notation.

```{r,echo=FALSE}
profils = data.frame(table(DF.tmp))
```

On ne garde que les profils qui comptent plus de 30 personnes afin d'avoir des résultats intéressants a analyser.

On compte maintenant 38 profils dont on donne la liste ci-dessous.

```{r, echo=FALSE}
profils = profils[profils[,5]>=30,]
# On renumérote correctement les lignes du dataframe.
row.names(profils) = paste(1:dim(profils)[1])
paged_table(profils)
```

```{r, echo=FALSE}
# On créer un liste de tout les profils.
profils = paste(profils[,1], profils[,2], profils[,3], profils[,4], sep="/")
# On recupère les notes.
DF.notes = DF.train[,7:20]
col_name = colnames(DF.notes)
# On attribue un profil à chaque individu.
DF.notes = cbind(paste(DF.tmp[,1], DF.tmp[,2], DF.tmp[,3], DF.tmp[,4], sep="/"), DF.notes)
# On renomme correctement les colonnes.
colnames(DF.notes) = c("profil", col_name)
```

On fait la moyenne des notes de chaque profil :

```{r,echo=FALSE}
bool_profil = DF.notes$profil %in% profils
# On récupère les profils qui sont dans notre dataframe, car,
# n'oublions pas qu'on n'étudie pas les profils regroupant moins de 30 personnes.
DF.notes=DF.notes[bool_profil,]
# Moyenne des notes de chaque profil.
DF.agg=aggregate.data.frame(x=DF.notes[,-1], by=list(DF.notes$profil), FUN=mean)[,-1]
row.names(DF.agg)=profils
paged_table(DF.agg)
```


### CAH


On commence par faire un clustering hiérarchique pour déterminer le nombre de groupes ($K$) idéal.

$K=6$ est un bon compromis.

```{r}
K=6
# On applique la méthode euclidienne pour avoir les meilleures distances en 2D
DF.agg.dist = dist(DF.agg, method="euclidean")
cah.ward = hclust(DF.agg.dist, method="ward.D2")
# On ajuste la taille des caractères pour un meilleur affichage 
par(cex.main=3, cex=0.4)
plot(cah.ward, hang=-1)
rect.hclust(cah.ward,K)
```

On a donc réussi à créer 6 groupes de profils similaires sur les 38 profils qu'on est en train d'étudier.

Penchons nous sur la composition de ces groupes et analysons les graces aux silhouettes:

```{r, echo=FALSE}
groupes.cah <-cutree(cah.ward, K)
sil=silhouette(groupes.cah,DF.agg.dist)
plot(sil, col=rainbow(K), main="\t\tSilhouettes")
rownames(sil)=rownames(DF.agg)
DF.tmp=data.frame(sil[,c(1,2,3)])
paged_table(DF.tmp[order(DF.tmp[,1]),])
```

Dans le tableau ci-dessus on peut voir que :

* Le groupe 1 ne rassemble que des individus en business class.

* Le groupe 2 réunit en grande majorité les individus en business class voyageant pour le business.

* Le groupe 3 regroupe les clients fidèles.

* Le groupe 4 est particulier et rassemble seulement deux profils, en particulier, des profils déloyaux qui voyagent en classe éco.

* Le groupe 5 réunit des passagers majoritairement en dessous de 39 ans voyageant en classe éco ou éco plus.

* Le groupe 6 regroupe les passagers de 39 ans et plus, voyageant en éco ou éco plus.

On remarque qu'en moyenne les groupes sont assez bien séparés, en effet, les silhouettes moyennes par groupe ne tombent jamais en dessous de 0.38 et la moyenne des silhouettes est de 0.46.

On peut voir que certain profils comme :

* Loyal Customer/61+/Personal Travel/Business
* Loyal Customer/7-19/Personal Travel/Eco	
* disloyal Customer/7-19/Business travel/Eco Plus

ont des silhouettes proche de 0, aux alentour de 0.15, ainsi ces profils chevauchent un ou plusieur groupes mais reste bien classés car leur silhouette est positive, on peut se référer au tableau dans la colonne "neighbor" pour voir quel groupe est le plus proche de leur profil.

***

```{r, echo=FALSE}
groupes.cah <-cutree(cah.ward, K)

Means_groupes.cah <- matrix(NA, nrow=K, ncol=dim(DF.notes)[2])
colnames(Means_groupes.cah)=c(colnames(DF.notes[,-1]),"satisfied")
rownames(Means_groupes.cah)= paste("Groupe",1:K,sep="_")
DF.tmp = data.frame(DF.notes,(DF.train[bool_profil,23]=="satisfied")*1)
for (i in 1:K) Means_groupes.cah[i,] <- colMeans(DF.tmp[DF.tmp$profil %in% rownames(data.frame(groupes.cah[groupes.cah==i])),][,-1])
```

Il n'est pas nécessaire de décrire toutes les moyennes des notes par cluster.

On va s'intéresser à la fréquence des individus satisfaits par cluster.

On voit que les fréquences diffèrent selon les groupes, ainsi, certains groupes sont en moyenne plus satisfait que d'autres.

Par exemple, le cluster n°3 possède la fréquence de satisfaction la plus élevé avec près de 73%, ainsi un individu ayant un profil associé à ce cluster aura tendance à être satisfait. À l'inverse, un passager dont le profil appartient au cluster n°4, où la fréquence de satisfaction est de 7%, aura tendance à être neutre ou insatisfait.

On a donc réussi à trouver des groupes de profils qui diffèrent par leur notes et ainsi la fréquence de satisfaction des individus qui le constituent.

```{r, echo=FALSE}
paged_table(data.frame(Means_groupes.cah))
```

Le clustering hiérarchique nous permet de déterminer le nombre de groupes qui semble idéal : $K = 6$. Les notes attribuées selon les différents groupes ne sont pas significativement différentes.


### Kmeans


On peut vérifié si le $K=6$ mis en évidence par le clustering cah était un bon compromis.

En effet, on voit que la diminution de l'inertie intra-classe est moins significative à partir de $K=6$, on aurait pu prendre $K$ un peu plus grand, mais pourquoi s'embêter avec un nombre de clusters trop élevé quand l'objectif est de regrouper au maximum sans perdre trop d'inertie.

```{r, echo=FALSE}
inertie.intra <-rep(0,times=37)
for(k in 1:37){
  kmeans.result <-kmeans(DF.agg,centers=k,nstart=100)
  inertie.intra[k] <- kmeans.result$tot.withinss/kmeans.result$totss
}

plot(1:37,inertie.intra,type="b",xlab="K",ylab="Inertie intra-classe",
     main="Pourcentage d'inertie intra-classe selon le nombre de classe")
abline(v=6, col="red")
```

On réalise ensuite le K-means, avec le meme nombre de groupes qu'avec le CAH.

```{r, echo=FALSE}
res.kmeans = kmeans(DF.agg, centers=Means_groupes.cah[,-15])
groupes.kmeans = res.kmeans$cluster

Means_groupes.kmeans <- matrix(NA, nrow=K, ncol=dim(DF.notes)[2])
colnames(Means_groupes.kmeans)=c(colnames(DF.notes[,-1]),"satisfied")
rownames(Means_groupes.kmeans)= paste("Groupe",1:K,sep="_")
DF.tmp = data.frame(DF.notes,(DF.train[bool_profil,23]=="satisfied")*1)
for (i in 1:K) Means_groupes.kmeans[i,] <- colMeans(DF.tmp[DF.tmp$profil %in% rownames(data.frame(groupes.kmeans[groupes.kmeans==i])),][,-1])
```

On voit que l'algorithme du k-means nous donne les mêmes groupes que celui du clustering hiérarchique.

```{r, echo=FALSE}
table(groupes.cah,groupes.kmeans)
```

Vérifions :

```{r, echo=FALSE}
paged_table(data.frame(Means_groupes.kmeans))
```


On représente les clusters en 2D à l'aide des composantes principales. On remarque que certains clusters se chevauchent, cette représentation montre l'emplacement de chacun des profils et du groupe auquel il appartient. 

On remarque qu'un groupe numéro 3 est isolé du reste des observations, c'est celui possédant le plus faible taux de satisfaction (0.07%). Sur le graphe des principales composantes (axes 1 et 2), ce groupe se situe à gauche, or on a vu dans l'ACP que les individus les moins satisfait se situaient bel et bien à gauche de l'axe 1.


```{r, echo=FALSE}
fviz_cluster(res.kmeans, data = DF.agg,
             palette = rainbow(K), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw(),
             main = "Kmeans : Représentation des clusters")
```


## Classification supervisé

### CART

Nous allons maintenant utiliser les algorithmes de classification supervisée pour tenter de prédire la satisfaction des voyageurs. Penchons nous sur l'arbre de décision suffisamment élagué pour être lisible. 

On peut observer que les voyageurs ayant mis une note pour l'enregistrement en ligne inférieure à 4, et une note pour le wifi comprise entre 1 inclus et 4 exclus sont plus fréquemment neutre ou insatisfait, seulement 6% d'entre eux sont satisfaits. De plus, ils représentent 44% du jeu d'entrainement. 

Un voyageur ayant noté à 4 ou 5 l'enregistrement en ligne, qui voyage pour des raison personnelles et ayant noté 5 le Wifi, sera satisfait à 100%. Ils représentent 1% du jeu d'entrainement.

```{r}
res.tree <- rpart(Satisfaction~., data=DF.train)
rpart.plot(res.tree, type=0)
```
L'arbre de décision prédit correctement la satisfaction des voyageurs avec 88.4% de précision binaire.

```{r}
pred.class.tree <- predict(res.tree, newdata=DF.test[,-23], type="class")
pred.prop.tree  <- predict(res.tree, newdata=DF.test[,-23], type="prob")

score.tree = sum((pred.class.tree == DF.test[,23])*1)/length(pred.class.tree)
score.tree
```
Optimisons le paramètre de complexité (cp) afin d'avoir une élagation optimale pour l'arbre et d'ainsi minimiser le risque de surapprentissage.

On remarque que l'arbre est illisible, on pourra quand même trouver le score de ce modèle optimisé.

```{r}
res.tree.opt <- rpart(Satisfaction~.,DF.train,control=rpart.control(minsplit = 50 ,cp=0))
cp.opt <- res.tree.opt$cptable[which.min(res.tree.opt$cptable[, "xerror"]), "CP"]
res.tree.opt <- prune(res.tree.opt,cp=cp.opt)
rpart.plot(res.tree.opt, type=0)
```
L'arbre de décision optimisé a un score de 95.5%.

```{r}
pred.class.tree.opt <- predict(res.tree.opt, newdata=DF.test[,-23], type="class")
pred.prob.tree.opt <- predict(res.tree.opt, newdata=DF.test[,-23], type="prob")
score.tree.opt = sum((pred.class.tree.opt==DF.test[,23])*1)/length(pred.class.tree.opt)
score.tree.opt
```
### LDA / QDA

On utilise les méthodes de classification supervisée pour prédire le comportement de notre data.test.

Avec ces méthodes, on tente de prédire la satisfaction des clients.

On crée nos modèles : 
```{r}
res.lda <- lda(Satisfaction~.,data=DF.train[,c(7:20,23)])
res.qda <- qda(Satisfaction~.,data=DF.train[,c(7:20,23)])
```


Le score obtenu par la méthode lda est de 81.3%
```{r}
# --- LDA
pred.class.lda <- predict(res.lda,  newdata = DF.test[,7:20])$class
pred.prob.lda  <- predict(res.lda,  newdata = DF.test[,7:20])$posterior[,2]
score.lda = sum(pred.class.lda==DF.test[,23])/length(pred.class.lda)
score.lda
```

Le score obtenu par la méthode qda est de 81.6%
```{r}
# --- QDA
pred.class.qda <- predict(res.qda,  newdata = DF.test[,7:20])$class
pred.prob.qda  <- predict(res.qda,  newdata = DF.test[,7:20])$posterior[,2]
score.qda = sum(pred.class.qda==DF.test[,23])/length(pred.class.qda)
score.qda
```
On lance une procédure de sélection de modèle avec une méthode de cross validation 5-fold. Le temps d'exécution de la méthode par défaut (10-fold) était trés long du à la taille du jeu de données. Afin de construire notre modèle nous nous appuyons "seulement" sur les 10 000 premiers individus, et non les 100 000, car sinon la selection de modèle est très longue.

```{r ATTENTION CELLULE LONGUE PRENEZ UN CAFÉ}
res.stepwise.lda=stepclass(Satisfaction ~., data=DF.train[1:10000,c(7:20,23)] , method="lda", direction="backward", fold=5)
res.stepwise.qda=stepclass(Satisfaction ~., data=DF.train[1:10000,c(7:20,23)] , method="qda", direction="backward", fold=5)

res.stepwise.lda = lda(res.stepwise.lda$formula , data = DF.train[1:10000,c(7:20,23)] )
res.stepwise.qda = qda(res.stepwise.qda$formula , data = DF.train[1:10000,c(7:20,23)] )
```


On prédit la satisfaction des passagers 

```{r}
pred.stepwise.lda <- predict(res.stepwise.lda,newdata=DF.test[,c(7:20,23)])$posterior[,2]
pred.stepwise.qda <- predict(res.stepwise.qda,newdata=DF.test[,c(7:20,23)])$posterior[,2]
```


La séléction de modèle avec lda a un score de 81.3% 

```{r}
pred.prop.stepwise.lda <- predict(res.stepwise.lda,newdata=DF.test[,c(7:20,23)])$posterior[,2]
score.stepwise.lda = sum(round(pred.stepwise.lda)==(DF.test[,c(7:20,23)]$Satisfaction =="satisfied")*1)/dim(DF.test)[1]
score.stepwise.lda
```

La séléction de modèle avec qda a un score de 84.5%

```{r}
pred.prop.stepwise.qda <- predict(res.stepwise.qda,newdata=DF.test[,c(7:20,23)])$posterior[,2]
score.stepwise.qda <- sum(round(pred.prop.stepwise.qda)==(DF.test[,c(7:20,23)]$Satisfaction =="satisfied")*1)/dim(DF.test)[1]
score.stepwise.qda
```


### RandomForest

On obtient un score aux alentours de 96% avec le Random Forest

```{r}
forest=randomForest(factor(DF.train$Satisfaction)~., DF.train, mtry=5, ntree = 15)
pred.class.RF=predict(forest, newdata=DF.test, type="class")
pred.prob.RF = predict(forest, newdata=DF.test, type="prob")
score.RF=sum(pred.class.RF==DF.test[,23])/length(pred.class.RF)
score.RF
```



```{r,message=FALSE}
# ----------------------------------------------------------------------------
ROC.lda           <- roc(DF.test$Satisfaction, pred.prob.lda)
ROC.qda           <- roc(DF.test$Satisfaction, pred.prob.qda)
ROC.stepwise.lda  <- roc(DF.test$Satisfaction, pred.prop.stepwise.lda)
ROC.stepwise.qda  <- roc(DF.test$Satisfaction, pred.prop.stepwise.qda)
ROC.tree          <- roc(DF.test$Satisfaction, pred.prop.tree[,2])
ROC.tree.opt      <- roc(DF.test$Satisfaction, pred.prob.tree.opt[,2])
ROC.RF            <- roc(DF.test$Satisfaction, pred.prob.RF[,2])
```

On remarque, grâce aux courbes ROC, que les meilleurs modèles sont l'arbre de décision optimisé et la Random Forest. En effet, l'air sous la courbe ROC des ces deux modèles est plus élevé que celles des autres modèles. De plus ces modèles sont très bon car l'air sous leur courbe ROC se rapproche de 1.

```{r, echo=FALSE}
plot(ROC.lda, col=1)
plot(ROC.qda, add=TRUE, col=2)
plot(ROC.stepwise.lda, add=TRUE,col=3)
plot(ROC.stepwise.qda, add=TRUE, col = 4)
plot(ROC.tree, add=TRUE, col = 5)
plot(ROC.tree.opt, add=TRUE, col = 6)
plot(ROC.RF,add=TRUE, col = 7)
leg = c("LDA","QDA","LDA stepwise","QDA stepwise","Arbre","Arbre optimisé","Random Forest")
AUC = round(c(ROC.lda$auc,ROC.qda$auc,ROC.stepwise.lda$auc,ROC.stepwise.qda$auc,ROC.tree$auc,ROC.tree.opt$auc,ROC.RF$auc),3)*100
leg = paste(leg,paste(AUC,"%",sep=""),sep=" : AUC=")
legend("bottomright",col = 1:7, lwd=2, leg)
title("Courbe ROC des différents modèles")
```


Récapitulatif : 


```{r, echo=FALSE, }
Score = c(score.lda, score.qda, score.stepwise.lda, score.stepwise.qda, score.tree, score.tree.opt, score.RF)
tab <- data.frame(AUC, Score)

rownames(tab) = c("LDA","QDA","LDA stepwise","QDA stepwise","Arbre","Arbre optimisé","Random Forest")
paged_table(tab)
```










